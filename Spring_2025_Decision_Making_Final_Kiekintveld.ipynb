{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIUQLIDkGRpu"
      },
      "source": [
        "# Part 1: Developing an agent to play Connect-4 using Tabular Q-Learning\n",
        "\n",
        "### Note that this is using only default random rollouts (pre-objective 1)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "Gv0hIRpf5r59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a class to handle reading a game configuration from a file\n",
        "class File_Reader:\n",
        "    # Constructor initializes the object with a path to the file\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path  # Store the file path as an instance variable\n",
        "\n",
        "    # Method to read the contents of the file and parse them\n",
        "    def read_file(self):\n",
        "        try:\n",
        "            # Open the file in read mode\n",
        "            with open(self.file_path, 'r') as file:\n",
        "                lines = file.readlines()  # Read all lines into a list\n",
        "\n",
        "                # First line contains the algorithm type (e.g., UR, UCT, Q-Learning, Deep Q-Learning)\n",
        "                algorithm_type = lines[0].strip()\n",
        "\n",
        "                # Second line contains the player's color (e.g., Red, Yello)\n",
        "                player_color = lines[1].strip()\n",
        "\n",
        "                # The next 6 lines represent the game board (6 rows)\n",
        "                board_lines = lines[2:8]\n",
        "\n",
        "                # Strip each line and convert it into a list of characters\n",
        "                board = [list(row.strip()) for row in board_lines]\n",
        "\n",
        "                # Return the parsed data as a tuple\n",
        "                return algorithm_type, player_color, board\n",
        "\n",
        "        # Handle case where the file does not exist\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File {self.file_path} not found.\")\n",
        "            return None\n",
        "\n",
        "        # Handle other unforeseen exceptions\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            return None\n"
      ],
      "metadata": {
        "id": "dnl3DpdV4eAk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "V7JBhnVqgN_b"
      },
      "outputs": [],
      "source": [
        "# Constants defining the game dimensions\n",
        "ROWS = 6          # Number of rows in the board\n",
        "COLUMNS = 7       # Number of columns in the board\n",
        "\n",
        "# Mapping symbols to integers for internal representation\n",
        "SYMBOLS = {\n",
        "    'o': 0,        # Empty cell\n",
        "    'r': 1,        # Red player\n",
        "    'y': 2,        # Yellow player\n",
        "    'red': 1,      # Alternative red input\n",
        "    'yellow': 2    # Alternative yellow input\n",
        "}\n",
        "\n",
        "# Reverse mapping from integers to display symbols\n",
        "INT_TO_SYMBOL = {\n",
        "    0: 'O',        # Empty\n",
        "    1: 'R',        # Red\n",
        "    2: 'Y'         # Yellow\n",
        "}\n",
        "\n",
        "# Q-table for reinforcement learning (can store state-action values)\n",
        "Q_table = {}\n",
        "\n",
        "# Class representing the game board\n",
        "class Board:\n",
        "    def __init__(self, rows=6, cols=7):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        # Initialize the board with 'O' to represent empty cells\n",
        "        self.board = [['O' for _ in range(cols)] for _ in range(rows)]\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the board to an empty state.\"\"\"\n",
        "        self.board = [['O' for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "\n",
        "    def copy(self):\n",
        "        \"\"\"Return a deep copy of the board.\"\"\"\n",
        "        new_board = Board(self.rows, self.cols)\n",
        "        # Deep copy each row to avoid shared references\n",
        "        new_board.board = [row.copy() for row in self.board]\n",
        "        return new_board\n",
        "\n",
        "    def PrintBoard(self):\n",
        "        \"\"\"Print the current state of the board in a readable format.\"\"\"\n",
        "        for row in self.board:\n",
        "            print('|'.join(row))  # Use pipe character to separate columns\n",
        "        print('-' * (self.cols * 2 - 1))  # Print separator line below board\n",
        "\n",
        "    def AvailableColumns(self):\n",
        "        \"\"\"Return a list of column indices that are not full (can accept a move).\"\"\"\n",
        "        return [col for col in range(self.cols) if self.board[0][col] == 'O']\n",
        "\n",
        "    def AvailableRowInColumn(self, column):\n",
        "        \"\"\"Return the first available row index from the bottom in the given column.\"\"\"\n",
        "        for row in reversed(range(self.rows)):  # Check from bottom to top\n",
        "            if self.board[row][column] == 'O':\n",
        "                return row\n",
        "        return -1  # Return -1 if the column is full\n",
        "\n",
        "    def CheckWin(self, player):\n",
        "        \"\"\"Check if the specified player ('R' or 'Y') has won the game.\"\"\"\n",
        "\n",
        "        # Check horizontal sequences\n",
        "        for r in range(self.rows):\n",
        "            for c in range(self.cols - 3):\n",
        "                if all(self.board[r][c + i] == player for i in range(4)):\n",
        "                    return True\n",
        "\n",
        "        # Check vertical sequences\n",
        "        for r in range(self.rows - 3):\n",
        "            for c in range(self.cols):\n",
        "                if all(self.board[r + i][c] == player for i in range(4)):\n",
        "                    return True\n",
        "\n",
        "        # Check diagonal from bottom-left to top-right\n",
        "        for r in range(self.rows - 3):\n",
        "            for c in range(self.cols - 3):\n",
        "                if all(self.board[r + i][c + i] == player for i in range(4)):\n",
        "                    return True\n",
        "\n",
        "        # Check diagonal from top-left to bottom-right\n",
        "        for r in range(3, self.rows):\n",
        "            for c in range(self.cols - 3):\n",
        "                if all(self.board[r - i][c + i] == player for i in range(4)):\n",
        "                    return True\n",
        "\n",
        "        return False  # No win found\n",
        "\n",
        "    def StateToKey(self):\n",
        "        \"\"\"\n",
        "        Convert the current board state into a hashable tuple of tuples,\n",
        "        using integer symbols, suitable for use as keys in a dictionary.\n",
        "        \"\"\"\n",
        "        return tuple(tuple(SYMBOLS.get(cell.lower(), 0) for cell in row) for row in self.board)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import NumPy for numerical operations (used here for random number generation)\n",
        "import numpy as np  # type: ignore\n",
        "\n",
        "# Dummy reinforcement learning policy for testing or placeholder purposes\n",
        "class Dummy_RL_Policy:\n",
        "    def predict(self, board):\n",
        "        \"\"\"\n",
        "        Generate a dummy prediction for the given board.\n",
        "\n",
        "        Returns a NumPy array of random values (one for each column),\n",
        "        simulating the output of a policy that rates each possible move.\n",
        "        \"\"\"\n",
        "        return np.random.rand(board.cols)  # Random values between 0 and 1 for each column\n"
      ],
      "metadata": {
        "id": "FuKjYR7x4YT7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import random          # For random move selection\n",
        "import copy            # For deep copying board states\n",
        "import math            # For exploration/exploitation calculations\n",
        "\n",
        "# Class representing a node in the Monte Carlo Tree Search using UCT (Upper Confidence Bound applied to Trees)\n",
        "class UCT_Node:\n",
        "    def __init__(self, board, player, parent=None):\n",
        "        self.board = copy.deepcopy(board)       # Deep copy of the game board to maintain state\n",
        "        self.player = player                    # Player associated with this node ('R' or 'Y')\n",
        "        self.parent = parent                    # Reference to the parent node (None for root)\n",
        "        self.children = {}                      # Dictionary mapping moves to child nodes\n",
        "        self.visits = 0                         # Number of times this node has been visited\n",
        "        self.q_value = 0                        # Total accumulated reward (win/loss signal)\n",
        "        self.untried_moves = board.AvailableColumns()  # Moves not yet explored from this node\n",
        "        self.q_values = None                    # Placeholder for storing values from an RL policy (optional)\n",
        "\n",
        "    def expand(self, rl_policy):\n",
        "        \"\"\"\n",
        "        Expand the current node by randomly selecting one of the untried moves.\n",
        "        Create a new child node corresponding to the move and assign predicted Q-values.\n",
        "        \"\"\"\n",
        "        move = random.choice(self.untried_moves)       # Randomly choose an unexplored move\n",
        "        new_board = copy.deepcopy(self.board)          # Create a new board for the child node\n",
        "        self.board.AvailableRowInColumn(move)          # (Seems unused here — may be a mistake or missing logic)\n",
        "        next_player = 'Y' if self.player == 'R' else 'R'  # Switch to the other player\n",
        "        child_node = UCT_Node(new_board, next_player, parent=self)  # Create child node\n",
        "        child_node.q_values = rl_policy.predict(new_board)          # Get Q-values from RL policy\n",
        "        self.children[move] = child_node                # Link move to the child node\n",
        "        self.untried_moves.remove(move)                 # Mark move as tried\n",
        "        return child_node\n",
        "\n",
        "    def best_child(self, c_param=1.4):\n",
        "        \"\"\"\n",
        "        Select the best child node using the UCT formula:\n",
        "        score = (exploitation term) + c_param * (exploration term)\n",
        "        \"\"\"\n",
        "        choices = []\n",
        "        for move, child in self.children.items():\n",
        "            exploit = child.q_value / (child.visits + 1e-8)  # Average reward\n",
        "            explore = math.sqrt(math.log(self.visits + 1) / (child.visits + 1e-8))  # UCB exploration term\n",
        "            score = exploit + c_param * explore\n",
        "            choices.append((score, move, child))\n",
        "        _, move, best = max(choices)  # Choose the child with the highest score\n",
        "        return best\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        \"\"\"Return True if all possible moves from this node have been explored.\"\"\"\n",
        "        return len(self.untried_moves) == 0\n",
        "\n",
        "    def is_terminal(self):\n",
        "        \"\"\"\n",
        "        Return True if the game is over: either player has won or no more moves are available.\n",
        "        \"\"\"\n",
        "        return self.board.CheckWin('R') or self.board.CheckWin('Y') or not self.board.AvailableColumns()"
      ],
      "metadata": {
        "id": "4nskVSpI4kuJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from uct_node import UCT_Node\n",
        "import random\n",
        "import numpy as np  # type: ignore\n",
        "import copy\n",
        "\n",
        "# Class representing the UCT search tree for Monte Carlo Tree Search (MCTS)\n",
        "class uct_tree:\n",
        "    def __init__(self, player, rl_policy, board, num_simulations=1000):\n",
        "        self.player = player                            # Player for whom the tree is searching moves ('R' or 'Y')\n",
        "        self.rl_policy = rl_policy                      # Reinforcement learning policy used to guide rollouts\n",
        "        self.num_simulations = num_simulations          # Number of simulations to run for MCTS\n",
        "        self.exploration_weight = 1.0                   # UCT exploration constant (not used directly here)\n",
        "\n",
        "        self.board = board                              # Store the initial board\n",
        "\n",
        "        # Create the root node using the current board and player\n",
        "        self.root = UCT_Node(self.board, self.player)\n",
        "        self.root.q_values = rl_policy.predict(self.board)  # Initialize root with Q-values from the RL policy\n",
        "\n",
        "    def search(self, board):\n",
        "        # Initialize a new root node for the current board state\n",
        "        root = UCT_Node(board, self.player)\n",
        "\n",
        "        # Check if the game is already over (no moves available)\n",
        "        if not board.AvailableColumns():\n",
        "            print(\"Game over: Board is full.\")\n",
        "            return None  # Could return a specific \"game over\" signal if needed\n",
        "\n",
        "        # Run simulations to build the tree and evaluate moves\n",
        "        for _ in range(self.num_simulations):\n",
        "            node_to_expand = self.select_node(root)           # Traverse the tree to select a node to expand\n",
        "            self.expand_node(node_to_expand)                  # Expand that node by adding a child\n",
        "            reward = self.rollout(node_to_expand.board, self.player)  # Simulate a game from that node\n",
        "            self.backpropagate(node_to_expand, reward)        # Propagate the result back up the tree\n",
        "\n",
        "        # Choose the best move based on the most visited child\n",
        "        if not root.children:\n",
        "            print(\"Warning: No children in root node, returning a random move.\")\n",
        "            return random.choice(board.AvailableColumns())  # Fallback in case of no expanded children\n",
        "\n",
        "        # Return the move that led to the child with the most visits\n",
        "        best_move = max(root.children.items(), key=lambda item: item[1].visits)[0]\n",
        "        return best_move\n",
        "\n",
        "    def select_node(self, node):\n",
        "        \"\"\"\n",
        "        Traverse the tree from the given node, selecting best children until an expandable or terminal node is found.\n",
        "        \"\"\"\n",
        "        while not node.is_terminal():\n",
        "            if not node.is_fully_expanded():\n",
        "                return node.expand(self.rl_policy)  # Expand if possible\n",
        "            else:\n",
        "                node = node.best_child()            # Otherwise keep selecting the best child\n",
        "        return node\n",
        "\n",
        "    def expand_node(self, node):\n",
        "        \"\"\"\n",
        "        Expands the given node by one child if not fully expanded.\n",
        "        \"\"\"\n",
        "        if not node.is_fully_expanded():\n",
        "            return node.expand(self.rl_policy)\n",
        "        return None\n",
        "\n",
        "    def rollout(self, board, player):\n",
        "        \"\"\"\n",
        "        Simulate a random game from the current board state using the RL policy to select moves.\n",
        "        Returns +1 if self.player wins, -1 if they lose, and 0 for a draw.\n",
        "        \"\"\"\n",
        "        rollout_board = copy.deepcopy(board)\n",
        "        turn = player\n",
        "\n",
        "        while True:\n",
        "            # Check if someone has won\n",
        "            if rollout_board.CheckWin('R'):\n",
        "                return 1 if self.player == 'R' else -1\n",
        "            if rollout_board.CheckWin('Y'):\n",
        "                return 1 if self.player == 'Y' else -1\n",
        "\n",
        "            available_moves = rollout_board.AvailableColumns()\n",
        "            if not available_moves:\n",
        "                return 0  # Draw if no more moves\n",
        "\n",
        "            # Use RL policy to predict Q-values and choose the best move\n",
        "            q_values = self.rl_policy.predict(rollout_board)\n",
        "            best_move = np.argmax(q_values)\n",
        "\n",
        "            # Determine the row to place the piece\n",
        "            row = rollout_board.AvailableRowInColumn(best_move)\n",
        "            if row != -1:  # Ensure valid row (i.e., column not full)\n",
        "                rollout_board.board[row][best_move] = 'R' if turn == 'R' else 'Y'\n",
        "\n",
        "            # Switch turn\n",
        "            turn = 'Y' if turn == 'R' else 'R'\n",
        "\n",
        "    def backpropagate(self, node, reward):\n",
        "        \"\"\"\n",
        "        Propagate the result of the rollout back through the path of selected nodes.\n",
        "        Alternate the reward (negate it) as it switches players.\n",
        "        \"\"\"\n",
        "        while node is not None:\n",
        "            node.visits += 1\n",
        "            node.q_value += reward\n",
        "            reward = -reward  # Switch perspective for the opposing player\n",
        "            node = node.parent\n"
      ],
      "metadata": {
        "id": "XhZH0aHP4rjN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from board import *  # Assuming necessary imports for board functionality\n",
        "import random\n",
        "\n",
        "class Uniform_Random:\n",
        "    @staticmethod\n",
        "    def UniformRandom(player, board, output_type):\n",
        "        \"\"\"\n",
        "        Simulates a game where players make random moves until the game ends.\n",
        "        Arguments:\n",
        "            player: Starting player ('R' or 'Y')\n",
        "            board: Board object representing the game state\n",
        "            output_type: Controls output verbosity ('verbose', 'none', etc.)\n",
        "        \"\"\"\n",
        "        opponent = 'Y' if player == 'R' else 'R'  # Determine the opponent\n",
        "        turn = player                             # Initialize whose turn it is\n",
        "        done = False                              # Game over flag\n",
        "\n",
        "        while not done:\n",
        "            if output_type.lower() == \"verbose\":\n",
        "                print(f\"Current board:\")\n",
        "                board.PrintBoard()               # Print board before the move (if verbose)\n",
        "\n",
        "            available_columns = board.AvailableColumns()  # Get list of valid columns to move in\n",
        "\n",
        "            if not available_columns:\n",
        "                print(\"Game over: Board is full.\")  # No moves left = draw\n",
        "                break\n",
        "\n",
        "            selected_column = random.choice(available_columns)  # Randomly choose a valid column\n",
        "            row = board.AvailableRowInColumn(selected_column)   # Get the row to drop the piece in\n",
        "            board.board[row][selected_column] = turn            # Place the piece on the board\n",
        "\n",
        "            print(f\"Move selected: {selected_column + 1}\\n\")     # Announce move (1-based column index)\n",
        "\n",
        "            if output_type.lower() != \"none\":\n",
        "                print(f\"Updated board:\")\n",
        "                board.PrintBoard()            # Show updated board (if output enabled)\n",
        "\n",
        "            # Check if the current player has won after the move\n",
        "            if board.CheckWin(turn):\n",
        "                print(f\"Game Over! {turn} Player wins.\")  # Announce winner\n",
        "                done = True\n",
        "            elif not board.AvailableColumns():             # Check if board is full after the move\n",
        "                print(\"Game Over! It's a draw.\")           # Announce draw\n",
        "                done = True\n",
        "            else:\n",
        "                turn = opponent if turn == player else player  # Alternate between player and opponent"
      ],
      "metadata": {
        "id": "xin1xuF84ubs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import deque\n",
        "import numpy as np  # type: ignore\n",
        "import matplotlib.pyplot as plt  # type: ignore\n",
        "from tensorflow.keras.models import Sequential  # type: ignore\n",
        "from tensorflow.keras.layers import Dense, Input  # type: ignore\n",
        "from tensorflow.keras.optimizers import Adam  # type: ignore\n",
        "# from board import *  # Assumes Board, ROWS, and COLUMNS are defined in board.py\n",
        "\n",
        "# Mapping symbols for the players\n",
        "SYMBOLS = {'r': 1, 'y': 2, 'red': 1, 'yellow': 2, 'R': 1, 'Y': 2}\n",
        "INT_TO_SYMBOL = {0: 'O', 1: 'R', 2: 'Y'}\n",
        "\n",
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    DQNAgent: A Deep Q-Network agent for training an AI to play a game like Connect4.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, action_size):\n",
        "        \"\"\"\n",
        "        Initializes the DQN agent.\n",
        "        Args:\n",
        "            state_size: The size of the state (number of cells in the board).\n",
        "            action_size: The number of actions (columns in the game).\n",
        "        \"\"\"\n",
        "        self.state_size = state_size  # Size of the state space\n",
        "        self.action_size = action_size  # Number of possible actions (columns)\n",
        "        self.memory = deque(maxlen=2000)  # Memory buffer for experience replay\n",
        "        self.gamma = 0.95  # Discount factor for future rewards\n",
        "        self.epsilon = 1.0  # Exploration rate (initially fully random)\n",
        "        self.epsilon_min = 0.01  # Minimum exploration rate\n",
        "        self.epsilon_decay = 0.995  # Decay factor for epsilon\n",
        "        self.learning_rate = 0.001  # Learning rate for the optimizer\n",
        "        self.model = self._build_model()  # Build the neural network model\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"\n",
        "        Builds the DQN model (a simple fully connected neural network).\n",
        "        Returns:\n",
        "            model: A compiled Keras model.\n",
        "        \"\"\"\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(self.state_size,)))  # Input layer with state size as input\n",
        "        model.add(Dense(24, activation='relu'))  # Hidden layer with 24 units and ReLU activation\n",
        "        model.add(Dense(24, activation='relu'))  # Another hidden layer\n",
        "        model.add(Dense(self.action_size, activation='linear'))  # Output layer with one unit per action\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))  # Compile the model with MSE loss and Adam optimizer\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Stores experiences in memory for future learning.\n",
        "        Args:\n",
        "            state: The current state.\n",
        "            action: The action taken.\n",
        "            reward: The reward received for the action.\n",
        "            next_state: The resulting state after the action.\n",
        "            done: Whether the game is over (True/False).\n",
        "        \"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))  # Store the experience\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Chooses an action based on the current state using an epsilon-greedy strategy.\n",
        "        Args:\n",
        "            state: The current state.\n",
        "        Returns:\n",
        "            action: The selected action (column).\n",
        "        \"\"\"\n",
        "        if np.random.rand() <= self.epsilon:  # Exploration\n",
        "            return random.randrange(self.action_size)  # Select a random action\n",
        "        act_values = self.model.predict(state, verbose=0)  # Predict Q-values for the current state\n",
        "        return np.argmax(act_values[0])  # Select the action with the highest Q-value\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        \"\"\"\n",
        "        Trains the model using a batch of experiences from memory.\n",
        "        Args:\n",
        "            batch_size: The number of experiences to sample for training.\n",
        "        \"\"\"\n",
        "        if len(self.memory) < batch_size:  # Ensure enough samples to replay\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, batch_size)  # Sample a batch from memory\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward  # Initialize the target with the immediate reward\n",
        "            if not done:\n",
        "                target += self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])  # Add future rewards if not done\n",
        "            target_f = self.model.predict(state, verbose=0)  # Get the current Q-values for the state\n",
        "            target_f[0][action] = target  # Update the Q-value for the taken action\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)  # Train the model for one epoch\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay  # Decay epsilon for less exploration over time\n",
        "\n",
        "    def load(self, name):\n",
        "        \"\"\"\n",
        "        Loads model weights from a file.\n",
        "        Args:\n",
        "            name: The file name to load weights from.\n",
        "        \"\"\"\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        \"\"\"\n",
        "        Saves model weights to a file.\n",
        "        Args:\n",
        "            name: The file name to save weights to.\n",
        "        \"\"\"\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "\n",
        "def TrainDQNAgent(player_color, EPISODES, board, make_plot=True):\n",
        "    \"\"\"\n",
        "    Trains a DQN agent to play the game.\n",
        "    Args:\n",
        "        player_color: The color of the player (used to map to 'R' or 'Y').\n",
        "        EPISODES: The number of training episodes.\n",
        "        board: The board object representing the game.\n",
        "        make_plot: Whether to generate training reward and epsilon plots.\n",
        "    Returns:\n",
        "        agent: The trained DQN agent.\n",
        "        rewards: The rewards obtained in each episode.\n",
        "        epsilon_values: The epsilon values during training.\n",
        "    \"\"\"\n",
        "    state_size = ROWS * COLUMNS  # Total number of cells on the board (flattened)\n",
        "    action_size = COLUMNS  # Number of possible actions (columns)\n",
        "    agent = DQNAgent(state_size, action_size)  # Create the DQN agent\n",
        "    batch_size = 32  # Mini-batch size for replay\n",
        "    rewards = []  # List to store rewards during training\n",
        "    epsilon_values = []  # List to store epsilon values during training\n",
        "\n",
        "    player_color = player_color.lower()  # Convert color to lowercase\n",
        "    if player_color not in SYMBOLS:\n",
        "        raise ValueError(f\"Invalid player color: {player_color}. Valid colors: 'R', 'Y', 'red', 'yellow'.\")\n",
        "    player = SYMBOLS[player_color]  # Map player color to symbol\n",
        "\n",
        "    for e in range(EPISODES):  # Loop through episodes\n",
        "        current_board = board.copy()  # Reset the board at the start of each episode\n",
        "        total_reward = 0  # Track total reward for the episode\n",
        "        done = False  # Track if the game is done\n",
        "\n",
        "        while not done:\n",
        "            state = np.reshape(current_board.StateToKey(), [1, state_size])  # Reshape board state for model input\n",
        "            action = agent.act(state)  # Choose an action based on the current state\n",
        "\n",
        "            # Check if the move is valid (column is not full)\n",
        "            if current_board.board[0][action] != 'O':\n",
        "                reward = -10  # Invalid move, penalize\n",
        "                done = True\n",
        "            else:\n",
        "                row = current_board.AvailableRowInColumn(action)  # Find the row to drop the piece\n",
        "                if row == -1:\n",
        "                    reward = -10  # Invalid move, penalize\n",
        "                    done = True\n",
        "                else:\n",
        "                    current_board.board[row][action] = INT_TO_SYMBOL[player]  # Update the board with the player's move\n",
        "                    if current_board.CheckWin(INT_TO_SYMBOL[player]):  # Check if player wins\n",
        "                        reward = 10  # Win, give positive reward\n",
        "                        done = True\n",
        "                    else:\n",
        "                        avail_cols = current_board.AvailableColumns()  # Check available columns\n",
        "                        if not avail_cols:  # If no more moves left, game is a draw\n",
        "                            reward = 0\n",
        "                            done = True\n",
        "                        else:\n",
        "                            opp_action = random.choice(avail_cols)  # Random move for opponent\n",
        "                            opp_row = current_board.AvailableRowInColumn(opp_action)\n",
        "                            current_board.board[opp_row][opp_action] = INT_TO_SYMBOL[3 - player]  # Opponent's move\n",
        "                            if current_board.CheckWin(INT_TO_SYMBOL[3 - player]):  # Check if opponent wins\n",
        "                                reward = -10  # Loss, penalize\n",
        "                                done = True\n",
        "                            else:\n",
        "                                reward = 0  # No winner, continue the game\n",
        "\n",
        "            next_state = np.reshape(current_board.StateToKey(), [1, state_size])  # Get the new state after the move\n",
        "            agent.remember(state, action, reward, next_state, done)  # Store the experience in memory\n",
        "            total_reward += reward  # Add the reward to the total reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            agent.replay(batch_size)  # Train the agent with experiences from memory\n",
        "\n",
        "        rewards.append(total_reward)  # Store the reward for the episode\n",
        "        epsilon_values.append(agent.epsilon)  # Store the epsilon value for the episode\n",
        "        print(f\"Episode: {e + 1}/{EPISODES}, Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
        "\n",
        "    # Optionally plot the training progress\n",
        "    if make_plot:\n",
        "        plt.plot(rewards)\n",
        "        plt.ylabel('Reward')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.title('DQN Agent Training Rewards')\n",
        "        plt.show()\n",
        "\n",
        "        plt.plot(epsilon_values)\n",
        "        plt.ylabel('Epsilon')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.title('DQN Agent Epsilon Over Time')\n",
        "        plt.show()\n",
        "\n",
        "    return agent, rewards, epsilon_values\n",
        "\n",
        "\n",
        "# Example: Initialize and print model summary\n",
        "if __name__ == \"__main__\":\n",
        "    dummy_board = Board()  # Create a dummy board object\n",
        "    agent = DQNAgent(state_size=ROWS * COLUMNS, action_size=COLUMNS)  # Initialize the DQN agent\n",
        "    agent.model.summary()  # Print model summary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "G15oK3nL419I",
        "outputId": "a893b14c-dc63-4f0f-de58-d6762d4a389d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │         \u001b[38;5;34m1,032\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │           \u001b[38;5;34m600\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m175\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">175</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,807\u001b[0m (7.06 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,807</span> (7.06 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,807\u001b[0m (7.06 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,807</span> (7.06 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt # type: ignore\n",
        "from IPython.display import clear_output  # For live updates in Jupyter environments\n",
        "# from board import *  # Assuming Board is defined in board.py\n",
        "\n",
        "class QAgent:\n",
        "    def __init__(self, Q_table=None, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):\n",
        "        \"\"\"\n",
        "        Initialize Q-learning agent with optional Q-table and hyperparameters.\n",
        "        \"\"\"\n",
        "        if Q_table is not None and not isinstance(Q_table, dict):\n",
        "            raise TypeError(\"Q_table must be a dictionary or None\")  # Ensure Q_table is a dictionary or None\n",
        "        self.Q_table = Q_table if Q_table else {}  # Initialize Q-table, if not provided\n",
        "        self.learning_rate = learning_rate  # Learning rate (alpha)\n",
        "        self.discount_factor = discount_factor  # Discount factor (gamma)\n",
        "        self.epsilon = epsilon  # Epsilon (for epsilon-greedy policy)\n",
        "\n",
        "    def StateToKey(self, board):\n",
        "        \"\"\"\n",
        "        Convert the current board state to a string key for Q-table lookup.\n",
        "        \"\"\"\n",
        "        return ''.join(''.join(row) for row in board.board)  # Join rows to create a unique state key\n",
        "\n",
        "    def QLearningMove(self, player, board):\n",
        "        \"\"\"\n",
        "        Choose the next move using epsilon-greedy policy, based on Q-values.\n",
        "        \"\"\"\n",
        "        state_key = self.StateToKey(board)  # Convert current board to state key\n",
        "        available_columns = board.AvailableColumns()  # Get available columns to play\n",
        "\n",
        "        # Initialize Q-table entries for the state if not already present\n",
        "        if state_key not in self.Q_table:\n",
        "            self.Q_table[state_key] = {col: 0.0 for col in available_columns}\n",
        "\n",
        "        # Select action based on epsilon-greedy strategy\n",
        "        selected_column = random.choice(available_columns) if random.random() < self.epsilon else \\\n",
        "                        max(available_columns, key=lambda col: self.Q_table[state_key].get(col, 0.0))\n",
        "\n",
        "        # Get the row where the piece should be placed in the selected column\n",
        "        row = board.AvailableRowInColumn(selected_column)\n",
        "\n",
        "        # Apply the move to the board if valid\n",
        "        if row != -1:\n",
        "            next_board = board.copy()  # Make a copy of the board to simulate the move\n",
        "            next_board.board[row][selected_column] = player  # Place the player's piece\n",
        "        else:\n",
        "            next_board = board  # If no valid row, return the original board\n",
        "\n",
        "        return next_board, selected_column  # Return updated board and chosen column\n",
        "\n",
        "    def TrainQLearning(self, player, num_episode, board, num_simulations=1, output_type=\"verbose\"):\n",
        "        \"\"\"\n",
        "        Train the Q-learning agent over multiple episodes.\n",
        "        \"\"\"\n",
        "        print(\"Training QLearning...\")\n",
        "        opponent = 'Y' if player == 'R' else 'R'  # Set the opponent color\n",
        "        win_rates = []  # To track win rates during training\n",
        "\n",
        "        for num_episode in range(num_simulations):\n",
        "            board.reset()  # Reset the board at the start of each simulation\n",
        "            done, turn = False, player  # Initialize game status and turn\n",
        "            history, reward = [], 0  # Initialize history and reward tracker\n",
        "\n",
        "            while not done:\n",
        "                state_key = self.StateToKey(board)  # Get state key for the current board\n",
        "                available_columns = board.AvailableColumns()  # Get available columns to play\n",
        "                if not available_columns:\n",
        "                    break  # If no available columns, exit the game\n",
        "\n",
        "                # Select move based on epsilon-greedy strategy (player's move)\n",
        "                next_board, action = self.QLearningMove(turn, board) if turn == player else (\n",
        "                    board.copy(), random.choice(available_columns))\n",
        "\n",
        "                next_state_key = self.StateToKey(board)  # Get the next state key after the move\n",
        "                history.append((state_key, action, next_state_key, reward))  # Track state-action transitions\n",
        "\n",
        "                board.board = next_board.board  # Update board with the new move\n",
        "\n",
        "                # Print board if output type is verbose\n",
        "                if output_type == \"verbose\":\n",
        "                    print(f\"After {turn}'s move (column {action + 1}):\")\n",
        "                    board.PrintBoard()\n",
        "\n",
        "                # Check win/loss conditions\n",
        "                if board.CheckWin(player):  # If player wins\n",
        "                    reward, done = 1, True\n",
        "                elif board.CheckWin(opponent):  # If opponent wins\n",
        "                    reward, done = -1, True\n",
        "                elif not available_columns:  # If it's a draw (no available columns)\n",
        "                    reward, done = 0, True\n",
        "\n",
        "                # Switch turns\n",
        "                turn = opponent if turn == player else player\n",
        "\n",
        "            # Backpropagate the reward through the state-action history\n",
        "            for state, action, next_state, reward in reversed(history):\n",
        "                self.Q_table.setdefault(state, {})\n",
        "                self.Q_table[state].setdefault(action, 0.0)\n",
        "                self.Q_table.setdefault(next_state, {})\n",
        "\n",
        "                # Update Q-value using the Bellman equation\n",
        "                max_future = max(self.Q_table[next_state].values(), default=0)\n",
        "                self.Q_table[state][action] += self.learning_rate * (reward + self.discount_factor * max_future - self.Q_table[state][action])\n",
        "                reward *= self.discount_factor  # Apply discount to the reward\n",
        "\n",
        "            # Print and plot win rates every 50 episodes\n",
        "            if num_episode % 50 == 0:\n",
        "                q_learning_win_rate = self.EvaluateAgent(player)  # Evaluate win rate\n",
        "                win_rates.append((num_episode, q_learning_win_rate, 0.5))  # Track win rates\n",
        "                self.PlotLearningCurve(win_rates, live_update=True)  # Plot learning curve\n",
        "                print(f\"Episode {num_episode}, Q-learning Win rate: {q_learning_win_rate:.2f}\")\n",
        "\n",
        "        print(\"Training completed.\")\n",
        "        self.PlotFinalResults(win_rates)  # Plot final results after training\n",
        "        return win_rates\n",
        "\n",
        "    def EvaluateAgent(self, player, num_games=20):\n",
        "        \"\"\"\n",
        "        Evaluate the agent's performance against a random agent.\n",
        "        \"\"\"\n",
        "        wins = 0  # Track number of wins for the agent\n",
        "        opponent = 'Y' if player == 'R' else 'R'  # Set the opponent color\n",
        "\n",
        "        for _ in range(num_games):\n",
        "            board = Board()  # Initialize a new game board\n",
        "            turn = player\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                available_columns = board.AvailableColumns()  # Get available columns to play\n",
        "                if not available_columns:\n",
        "                    break  # If no available columns, end the game\n",
        "\n",
        "                # Player's move (QLearning)\n",
        "                if turn == player:\n",
        "                    _, action = self.QLearningMove(player, board)\n",
        "                    row = board.AvailableRowInColumn(action)\n",
        "                    if row != -1:\n",
        "                        board.board[row][action] = player  # Make the move for the player\n",
        "                else:  # Opponent's move (random selection)\n",
        "                    action = random.choice(available_columns)\n",
        "                    row = board.AvailableRowInColumn(action)\n",
        "                    if row != -1:\n",
        "                        board.board[row][action] = opponent  # Make the move for the opponent\n",
        "\n",
        "                # Check for win/loss conditions\n",
        "                if board.CheckWin(player):  # If player wins\n",
        "                    wins += 1\n",
        "                    break\n",
        "                elif board.CheckWin(opponent) or not board.AvailableColumns():  # If opponent wins or draw\n",
        "                    break\n",
        "\n",
        "                # Switch turns\n",
        "                turn = opponent if turn == player else player\n",
        "\n",
        "        return wins / num_games  # Return win rate based on evaluation\n",
        "\n",
        "    def PlotLearningCurve(self, win_rates, live_update=False):\n",
        "        \"\"\"\n",
        "        Plot the learning curve of the agent's performance over time.\n",
        "        \"\"\"\n",
        "        episodes = [entry[0] for entry in win_rates]  # Extract episodes from win rates\n",
        "        q_learning_win_rates = [entry[1] for entry in win_rates]  # Extract Q-learning win rates\n",
        "        random_win_rates = [entry[2] for entry in win_rates]  # Extract random agent win rates\n",
        "\n",
        "        plt.figure(figsize=(8,5))\n",
        "        plt.plot(episodes, q_learning_win_rates, marker='o', color='red', label='Q-learning Agent')\n",
        "        plt.plot(episodes, random_win_rates, color='blue', label='Random Agent')\n",
        "        plt.title('Win Rates over Training')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Win Rate')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # If live update is enabled, clear and update the plot in real-time\n",
        "        if live_update:\n",
        "            clear_output(wait=True)\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    def PlotFinalResults(self, win_rates):\n",
        "        \"\"\"\n",
        "        Plot the final results of the agent's performance after training.\n",
        "        \"\"\"\n",
        "        q_learning_win_rate = self.EvaluateAgent('R')  # Evaluate Q-learning agent\n",
        "        random_win_rate = self.EvaluateAgent('Y')  # Evaluate random agent\n",
        "        win_rates.append((len(win_rates), q_learning_win_rate, random_win_rate))  # Append final results\n",
        "        print(f\"Final Q-learning Win rate: {q_learning_win_rate:.2f}\")\n",
        "        print(f\"Final Random Win rate: {random_win_rate:.2f}\")\n",
        "        self.PlotLearningCurve(win_rates, live_update=False)  # Plot final learning curve"
      ],
      "metadata": {
        "id": "Ywf6jAVc46dY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2ACoJBlntsr"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "# from file_reader import File_Reader\n",
        "# from board import Board\n",
        "# from uct_tree import uct_tree\n",
        "# from uniform_random import Uniform_Random\n",
        "# from dummy_rl_policy import Dummy_RL_Policy\n",
        "# from q_agent import QAgent\n",
        "#from uct_node import UCT_Node\n",
        "import numpy as np  # type: ignore\n",
        "import matplotlib.pyplot as plt  # type: ignore\n",
        "# from dqn_agent import *\n",
        "\n",
        "class Main:\n",
        "    SYMBOLS = {'B': 1, 'R': 2}  # Mapping for player symbols (B for Black, R for Red)\n",
        "    num_simulations = 5  # Default number of simulations for certain algorithms\n",
        "    game_loop = True  # Flag to control whether the game loop continues\n",
        "\n",
        "    def main(self):\n",
        "        print(\"Main class initialized\")\n",
        "\n",
        "        # Initialize file reader and read file content\n",
        "        self.file_reader = File_Reader(\"test.txt\")  # Create an instance of File_Reader\n",
        "        algorithm_type, player_color, board_data = self.file_reader.read_file()  # Read data from file\n",
        "        print(f\"Algorithm Type: {algorithm_type}\")\n",
        "        print(f\"Player Color: {player_color}\")\n",
        "        print(f\"Board Data: {board_data}\")\n",
        "\n",
        "        # Validate board data format\n",
        "        if not isinstance(board_data, list) or not all(isinstance(row, list) for row in board_data) or not all(isinstance(cell, str) for row in board_data for cell in row):\n",
        "            raise TypeError(\"Board data is not in the correct 2D list format!\")  # Raise error if data is not in proper format\n",
        "\n",
        "        # Initialize the board and set it to the board data from the file\n",
        "        self.board = Board(len(board_data), len(board_data[0]))  # Create a new Board object based on the board size\n",
        "        self.board.board = board_data  # Set the board data\n",
        "        self.board.PrintBoard()  # Print the initial board\n",
        "\n",
        "        # Start the game loop\n",
        "        while self.game_loop:\n",
        "            algorithm_type = input(\"Enter algorithm type (UR, UCT, QL, DQN, C to quit): \").strip()  # Prompt user for algorithm choice\n",
        "\n",
        "            # Validate the input for the algorithm type\n",
        "            if algorithm_type not in [\"UR\", \"UCT\", \"QL\", \"DQN\", \"C\"]:\n",
        "                print(f\"Invalid input: {algorithm_type}. Please try again.\")  # Invalid input, prompt again\n",
        "                continue\n",
        "\n",
        "            if algorithm_type == \"C\":\n",
        "                print(\"Exiting the game.\")  # Exit the game loop if 'C' is chosen\n",
        "                break\n",
        "\n",
        "            print(f\"\\n--- Running {algorithm_type} Algorithm ---\")\n",
        "\n",
        "            # Implement different game algorithms based on user choice\n",
        "\n",
        "            if algorithm_type == \"UR\":  # Uniform Random Algorithm\n",
        "                Uniform_Random.UniformRandom(player_color, self.board, output_type=\"verbose\")  # Call Uniform Random algorithm to make a move\n",
        "\n",
        "            elif algorithm_type == \"UCT\":  # Upper Confidence Bound for Trees (UCT) Algorithm\n",
        "                rl_policy = Dummy_RL_Policy()  # Placeholder policy\n",
        "                uct = uct_tree(player_color, rl_policy, self.board, num_simulations=self.num_simulations)  # Initialize UCT tree with given parameters\n",
        "                move = uct.search(self.board)  # Perform UCT search to find best move\n",
        "                row = self.board.AvailableRowInColumn(move)  # Get the row to place the piece in the selected column\n",
        "                if row != -1:\n",
        "                    self.board.board[row][move] = player_color  # Update the board with the move\n",
        "                print(f\"UCT selected column: {move + 1}\")\n",
        "\n",
        "            elif algorithm_type == \"QL\":  # Q-Learning Algorithm\n",
        "                rl_policy = Dummy_RL_Policy()  # Placeholder policy\n",
        "                q_agent = QAgent()  # Initialize Q-agent for training\n",
        "                q_agent.TrainQLearning(player_color, 1, self.board, num_simulations=self.num_simulations, output_type=\"verbose\")  # Train Q-agent\n",
        "                _, selected_column = q_agent.QLearningMove(player_color, self.board)  # Get the column to move based on trained Q-values\n",
        "                row = self.board.AvailableRowInColumn(selected_column)  # Get the row to place the piece in the selected column\n",
        "                if row != -1:\n",
        "                    self.board.board[row][selected_column] = player_color  # Update the board with the move\n",
        "                print(f\"Q-Learning selected column: {selected_column + 1}\")\n",
        "\n",
        "            elif algorithm_type == \"DQN\":  # Deep Q-Network Algorithm\n",
        "                print(f\"Training DQN Agent for {self.num_simulations} episodes...\")  # Print training message\n",
        "                agent, rewards, epsilon_values = TrainDQNAgent(player_color, self.num_simulations, self.board)  # Train DQN agent\n",
        "                state = np.reshape(self.board.board, [1, len(self.board.board) * len(self.board.board[0])])  # Reshape the board state\n",
        "                action = agent.act(state)  # Get the action (column) based on the trained DQN agent\n",
        "                row = self.board.AvailableRowInColumn(action)  # Get the row to place the piece in the selected column\n",
        "                player_num = self.SYMBOLS[player_color]  # Get the player number based on the color\n",
        "                if row != -1:\n",
        "                    self.board.board[row][action] = player_num  # Update the board with the move\n",
        "                print(f\"DQN selected column: {action + 1}\")\n",
        "\n",
        "                plt.show()  # Display the learning curve\n",
        "\n",
        "            # Print updated board after move\n",
        "            print(\"Updated board:\")\n",
        "            self.board.PrintBoard()\n",
        "\n",
        "            # Check if the current player has won or if the game is a draw\n",
        "            if self.board.CheckWin(player_color):\n",
        "                print(f\"Game Over! {player_color} Player wins.\")  # Player wins\n",
        "                self.game_loop = False  # End the game\n",
        "            elif not self.board.AvailableColumns():  # Check if there are no available moves left\n",
        "                print(\"Game Over! It's a draw.\")  # The game is a draw\n",
        "                self.game_loop = False  # End the game\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    runner = Main()  # Create an instance of the Main class\n",
        "    runner.main()  # Call the main method to start the game loop"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}